{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_DATA_FILE = 'SpamData\\02_Training\\train-data.txt'\n",
    "TEST_DATA_FILE = 'SpamData\\02_Training\\test-data.txt'\n",
    "VOCAB_SIZE = 2500\n",
    "# TOKEN_SPAM_PROB_FILE = ' save this file in a path' \n",
    "# TOKEN_HAM_PROB_FILE = 'Save the file'\n",
    "# TOKEN_ALL_PROB_FILE = 'Save the file'\n",
    "Test_Features_Matrix = 'SpamData\\03_Testing\\test-features.txt'\n",
    "Test_target_FIle = 'SpamData\\03_Testing\\prob-all-tokens.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_train_data = np.loadtxt(TRAINING_DATA_FILE, delimiter= ' ', dtype=int)\n",
    "sparse_test_data = np.loadtxt(TEST_DATA_FILE, delimiter=' ', dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_train_data[ : 5] # 4 categories in this table DOC_ID, Word_ID, Category, OCCURENCE\n",
    "sparse_test_data[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('No of rows in training file', sparse_train_data.shape[0])\n",
    "print('No of rows in test file', sparse_test_data.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to Create an Empty DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = ['DOC_ID']+['CATEGORY'] + list(range(0, VOCAB_SIZE)) # creating the column name items with categories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_names = np.unique(sparse_train_data[:, 0]) # selecting all rows with one column of data\n",
    "full_train_data = pd.DataFrame(index=index_names, columns=column_names)\n",
    "full_train_data.fillna(value=0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create full matrix from sparse matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_full_matrix(sparse_matrix, nr_words, doc_idx=0, word_idx = 1, categ_idx = 2, freq_idx = 3):\n",
    "\n",
    "       column_names = ['DOC_ID']+['CATEGORY'] + list(range(0, VOCAB_SIZE)) # creating the column name items with categories\n",
    "       doc_id_names = np.unique(sparse_train_data[:, 0])\n",
    "       full_matrix = pd.DataFrame(index=doc_id_names, columns=column_names)\n",
    "       full_matrix.fillna(value=0, inplace=True)\n",
    "\n",
    "       for i in range(sparse_matrix.shape[0]):\n",
    "              doc_nr = sparse_matrix[i][0]  # doc_idx denotes 1st column\n",
    "              word_id = sparse_matrix[i][1]  # word_idx denoted as 2 column here\n",
    "              label = sparse_matrix[i][2] # Likewise\n",
    "              occurence = sparse_matrix[i][freq_idx]\n",
    "\n",
    "              full_matrix.at[doc_nr, 'DOC_ID'] = doc_nr\n",
    "              full_matrix.at[doc_nr, 'CATEGORY'] = label\n",
    "              full_matrix.at['doc_nr', 'word_id'] = occurence\n",
    "              full_matrix.set_index['DOC_ID']\n",
    "       \n",
    "       return full_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Naives Bayes Model\n",
    "\n",
    "Calculating the probablity of Spam | Viagra, substitue into the formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_train_data.CATEGORY.size\n",
    "full_train_data.CATEGORY.sum() # to get the category of spam messages\n",
    "prob_spam = full_train_data.full_train_data.CATEGORY.sum()/full_train_data.CATEGORY.size\n",
    "print('Probablity of spam is', prob_spam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total no.of words / tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_train_features = full_train_data.loc[:, full_train_data.columns!= 'CATEGORY'] # selecting the columns \n",
    "#with word_id not the CATEGORY column\n",
    "full_train_features.head()\n",
    "# to get the total no.of words we have to sum each row in the table\n",
    "email_lengths = full_train_features.sum(axis=1) # sum up the columns\n",
    "TOTal_wordcount = email_lengths.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create a subset of email_lengths, that only contains the spam messg and then count tot no.of words in spam email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_length= email_lengths[full_train_data.CATEGORY == 1]\n",
    "spam_wc = spam_length.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_length= email_lengths[full_train_data.CATEGORY == 0]\n",
    "ham_length.sum()\n",
    "non_spam_wordcount = ham_length.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Average nr of words in spam mails'.format(spam_wc/spam_length.shape[0]))\n",
    "print('Average nr of words in ham mails'.format(non_spam_wordcount/ham_length.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summing the Tokens Occuring in Spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#full_train_data is with all columns including category column , full train features is without category of spam classification.\n",
    "train_spam_tokens = full_train_features.loc[full_train_data.CATEGORY == 1 ]\n",
    "train_spam_tokens.head() # spam hs 1259 so the overall shape should be 9(1249, 2500)\n",
    "\n",
    "summed_spam_tokens = train_spam_tokens.sum(axis=0) + 1 # while adding the no.of.\n",
    "#occurences if one of the row is zero , to make it as non-zero.\n",
    "\n",
    "# REPEAT the same sprocedure for summing the tokens in HAM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P( Token | SPam) - Probablity that a token occurs given the email is Spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_tokens_spam = summed_spam_tokens/(spam_wc + VOCAB_SIZE)\n",
    "prob_tokens_spam.sum() #--> this is P(Token | Spam)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " P( Token | SPam) - Probablity that a token occurs given the email is Spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_tokens_nonspam = summed_ham_tokens/(non_spam_wordcount + VOCAB_SIZE) # not defined for summed_ham_tokens\n",
    "prob_tokens_nonspam.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P(Token) - Probablity that token occurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_tokens_all = full_train_features.sum(axis=0) / TOTal_wordcount\n",
    "prob_tokens_all.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(TOKEN_SPAM_PROB_FILE, prob_tokens_spam) # ( path for file, objects created to use)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create full matrix from sparse test data with that func created, seperate feature and target values, save the files as test target file and test feature matrix file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_test_data = make_full_matrix(sparse_test_data, nr_words= VOCAB_SIZE)\n",
    "x_test = full_test_data.loc[:, full_test_data.columns != 'CATEGORY'] # feature\n",
    "y_test = full_test_data.CATEGORY  # target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(Test_target_FIle, y_test)\n",
    "np.savetxt(Test_target_FIle, x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calcualting joint probablity using dot product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=np.array([1,2,3])\n",
    "b=np.array([4,5,6])\n",
    "a.dot(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test.dot(prob_tokens_spam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set the prior-- it is guess or belief about some quantity\n",
    "$$ P(Spam \\, | \\, X) = \\frac{P(X \\, | \\, Spam  \\,) \\, P(Spam)} {P(X)} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROB_SPAM = 0.3116\n",
    "# taking log for calculation for very small values, taking log will spread the \n",
    "#values for plotting in graph, easy to intrepret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joint Probability in log format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_log_spam = x_test.dot(np.log(prob_tokens_spam) - np.log(prob_tokens_all)) + np.log(prob_tokens_nonspam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ P(ham \\, | \\, X) = \\frac{P(X \\, | \\, ham  \\,) \\, P(ham)} {P(X)} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_log_ham = x_test.dot(np.log(prob_tokens_nonspam) - np.log(prob_tokens_all)) + np.log(prob_tokens_nonspam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making Predictions \n",
    "Checking for higher joint probability\n",
    "$$P( spam\\, | \\, x) > P( ham \\, | x \\,) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Prediction = joint_log_spam > joint_log_spam"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
